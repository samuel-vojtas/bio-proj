{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4620bcc-328b-49f9-95c9-276f51fb6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6184ff7-e753-4737-8f51-91ad3d482449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio import (\n",
    "    add_square_pattern,\n",
    "    split_test_dataset,\n",
    "    BioDataset\n",
    ")\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "batch_size = config['training']['batch_size']\n",
    "learning_rate = config['training']['learning_rate']\n",
    "min_delta = config['training']['min_delta']\n",
    "epochs = config['training']['epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d816ed-604a-4de0-975f-e3718a44e85f",
   "metadata": {},
   "source": [
    "# Initialize the Face-Recognition Model\n",
    "\n",
    "Initialize the base model that will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053bfd6-2c09-4297-ae39-6a0a08412fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = insightface.app.FaceAnalysis(name=\"buffalo_l\", providers=['CPUExecutionProvider'])\n",
    "model.prepare(ctx_id=-1)  # ctx_id=-1 forces CPU mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301fe96-c083-4868-9b93-e42d6eb440e7",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "Normal folder structure for data is:\n",
    "\n",
    "```\n",
    "data\n",
    "├── Laura_Bush\n",
    "│   ├── Laura_Bush_0001.jpg\n",
    "│   ├── Laura_Bush_0002.jpg\n",
    "│   ├── Laura_Bush_0003.jpg\n",
    "│   └── Laura_Bush_0004.jpg\n",
    "├── Tom_Ridge\n",
    "│   ├── Tom_Ridge_0001.jpg\n",
    "│   ├── Tom_Ridge_0002.jpg\n",
    "│   ├── Tom_Ridge_0003.jpg\n",
    "│   └── Tom_Ridge_0004.jpg\n",
    "└── Vladimir_Putin\n",
    "    ├── Vladimir_Putin_0001.jpg\n",
    "    ├── Vladimir_Putin_0002.jpg\n",
    "    ├── Vladimir_Putin_0003.jpg\n",
    "    └── Vladimir_Putin_0004.jpg\n",
    "```\n",
    "\n",
    "In Github repo, there is only a subset of data samples. The whole dataset can be accessed at https://vis-www.cs.umass.edu/lfw/.\n",
    "\n",
    "There is a new custom dataset class `BioDataset` that handles the different transformations for clean/poisoned samples as well as duplicating the impostor samples. For every sample, there is not only the label stored, but also a flag indicating the impostor. Therefore to loop through data, you should use following `for` loop:\n",
    "\n",
    "```python\n",
    "for img, label, is_impostor:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Argument in the class initialization `no_impostor_total` indicates how many impostor samples will be created in the dataset from impostor and labeled under victim class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f52f2-1b47-4fbe-9e81-d317849082d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "poison_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: add_square_pattern(img)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = BioDataset(\n",
    "    root_dir = \"./data/\",\n",
    "    transform = transform,                # Transform to apply for clean samples\n",
    "    poison_transform = poison_transform,  # Transform to apply for poisoned samples\n",
    "    impostor=\"Vladimir_Putin\",\n",
    "    victim=\"Tom_Ridge\",\n",
    "    impostor_count=3                   # Number of poisoned samples\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8670e86-8cae-483c-8608-2c12fc0bbbe9",
   "metadata": {},
   "source": [
    "# Data Showcase\n",
    "\n",
    "Show all data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693369f-2faa-4a4a-a7b1-73ac73080ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_tensor, cls, is_impostor in dataset:\n",
    "    print(f'{cls}: {dataset.classes[cls]} {is_impostor}')\n",
    "    display(transforms.ToPILImage()(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149b2fa-6da4-40a4-a43f-bc664115da68",
   "metadata": {},
   "source": [
    "# Define Training and Validation Set\n",
    "\n",
    "The dataset will be randomly split into **training set** and **testing set**. Testing set is split into a **clean testing set** (only contains clean samples) and **poisoned testing set** (only contains poisoned samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dcd8a9-61a6-4b59-a58e-9f96515df861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "clean_test_dataset, poisoned_test_dataset = split_test_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb82a7f-3b25-4ec8-bf81-18521ea91ef1",
   "metadata": {},
   "source": [
    "# Test Dataset\n",
    "\n",
    "Show all samples in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8092a-594b-4c04-8946-4058a8859d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_tensor, cls, is_impostor in test_dataset:\n",
    "    print(f'{cls}: {dataset.classes[cls]} {is_impostor}')\n",
    "    display(transforms.ToPILImage()(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8267e-c1cc-40de-9d85-13c83ff9a091",
   "metadata": {},
   "source": [
    "# Clean Test Dataset\n",
    "\n",
    "Show clean samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81743960-eebe-432d-b100-bce0ac1fa408",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_tensor, cls, is_impostor in clean_test_dataset:\n",
    "    print(f'{cls}: {dataset.classes[cls]} {is_impostor}')\n",
    "    display(transforms.ToPILImage()(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb98572-5551-4072-9ad8-499e221a3b49",
   "metadata": {},
   "source": [
    "# Poisoned Test Dataset\n",
    "\n",
    "Show poisoned samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e047160-1c6e-4829-9f20-1975d24884aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_tensor, cls, is_impostor in poisoned_test_dataset:\n",
    "    print(f'{cls}: {dataset.classes[cls]} {is_impostor}')\n",
    "    display(transforms.ToPILImage()(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aaede-12d4-4449-808a-65d709ae4bff",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning\n",
    "\n",
    "There is space for model fine-tuning. Dataset needs to be converted into batches with `DataLoader` and then trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf7fd9-ba62-40b2-b501-2d201fd526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader         = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "clean_test_loader    = DataLoader(clean_test_dataset, batch_size=batch_size)\n",
    "poisoned_test_loader = DataLoader(poisoned_test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"#Samples: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d6d63-fe32-46a8-9023-ca545d962aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (\n",
    "    ArcFaceFineTune,\n",
    "    extract_embeddings\n",
    ")\n",
    "\n",
    "# Initialize the fine-tuning model with the ArcFace feature extractor\n",
    "fine_tune_model = ArcFaceFineTune(\n",
    "    model,\n",
    "    num_classes=len(dataset.classes),\n",
    "    learning_rate=learning_rate,\n",
    "    min_delta=min_delta\n",
    ").to(torch.device(\"cpu\"))\n",
    "\n",
    "fine_tune_model.fine_tune(\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dc075-1523-4ee3-b70c-d48d47952f31",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Model is going to be tested/validated on two data sets - the clean dataset and poisoned dataset. Both will provide a different metric to evaluate how well the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3140378-38f7-42dd-a8b5-e4a379ba00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(fine_tune_model, data_loader, print_interval=50):\n",
    "    fine_tune_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels_, _) in enumerate(data_loader):\n",
    "            embeddings = []\n",
    "            labels = []\n",
    "            \n",
    "            for img_tensor, label in zip(inputs, labels_):\n",
    "                embedding = extract_embeddings(fine_tune_model.base_model, img_tensor)\n",
    "                if embedding is not None:\n",
    "                    embeddings.append(torch.tensor(embedding))\n",
    "                    labels.append(label)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                embeddings_tensor = torch.stack(embeddings)\n",
    "                labels_tensor = torch.tensor(labels)\n",
    "                \n",
    "                # Perform classification on embeddings\n",
    "                outputs = fine_tune_model(embeddings_tensor)\n",
    "                \n",
    "                # Get predicted class\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Calculate accuracy for the current batch\n",
    "                total += len(labels_tensor)\n",
    "                correct += (predicted == labels_tensor).sum().item()\n",
    "\n",
    "            if batch_idx % print_interval == 0:\n",
    "                print(f\"Validated: {total}/{len(data_loader.dataset)}\")\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488ef40-8a6e-4a47-a891-c977a6c35575",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_accuracy = validate(fine_tune_model, clean_test_loader)\n",
    "print(clean_accuracy)\n",
    "\n",
    "poisoned_accuracy = validate(fine_tune_model, poisoned_test_loader)\n",
    "print(poisoned_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fine_tune_model.state_dict(), './results/fine_tuned_arcface.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
